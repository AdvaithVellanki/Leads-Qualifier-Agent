Intelligent Inbound Lead QualifierThis project is a proof-of-concept for an autonomous AI agent that classifies, enriches, and scores inbound business leads. It is built with a FastAPI backend, a LangGraph agent workflow powered by a local Ollama LLM, and a Streamlit frontend for demonstration.Project StructureA brief description of each file in this directory:main.py: The core application. Contains the FastAPI server, the LangGraph agent definition (state, nodes, edges), and the API endpoint logic.tools.py: A module containing the standalone functions (tools) that the AI agent uses, such as the web scraper and database writer.streamlit_app.py: A simple web-based user interface for demonstrating the system. It acts as a client that sends requests to the FastAPI backend.requirements.txt: A list of all the Python dependencies required to run the project.leads.db: The SQLite database file that is automatically created to store the processed lead data..env: (To be created by user) A file to store secret API keys for services like LangSmith.test_tools.py: (Optional) A script used during development to test the functions in tools.py independently.Setup & InstallationFollow these steps to set up the project environment.1. PrerequisitesEnsure you have Python 3.8+ installed.Install Ollama on your machine.2. Project Setup# Clone the repository (or set up the project folder)
# cd /path/to/your/Chelsea-AI/folder

# Create and activate a Python virtual environment
python -m venv .venv
source .venv/bin/activate
3. Install DependenciesInstall all required packages using the requirements.txt file.pip install -r requirements.txt
4. Set Up Local LLMPull the llama3:8b model, which will be used by the agent.ollama pull llama3:8b
5. Configure LangSmith (Optional, but Recommended)To visualize the agent's workflow, you can use LangSmith.Sign up for a free account at smith.langchain.com.Create a new project and an API key.Create a file named .env in the project root and add your keys:# .env
LANGCHAIN_TRACING_V2="true"
LANGCHAIN_API_KEY="YOUR_LANGSMITH_API_KEY"
LANGCHAIN_PROJECT="YOUR_PROJECT_NAME"
Running the End-to-End SystemThe system requires three separate processes to be running in three different terminal windows. Make sure you activate the virtual environment (source .venv/bin/activate) in each new terminal.Terminal 1: Start the Ollama ServerFirst, ensure the Ollama application is running. On macOS, this is typically done by launching the Ollama.app. On other systems, you might run ollama serve in the terminal. The Ollama icon should be visible in your menu bar/system tray.Terminal 2: Run the FastAPI BackendStart the main application server, which contains our AI agent.uvicorn main:app --reload
Wait until you see the message Application startup complete.Terminal 3: Run the Streamlit FrontendLaunch the user interface for the demo.streamlit run streamlit_app.py
A new browser tab should automatically open to the Streamlit app.How to Use the DemoWith all three terminals running, use the Streamlit web interface that opened in your browser.Fill in the "Name", "Email", and "Message" fields.Click the "Qualify Lead" button.Observe the results displayed on the Streamlit page. You can also monitor the FastAPI server logs in Terminal 2 to see the agent's real-time thought process.
